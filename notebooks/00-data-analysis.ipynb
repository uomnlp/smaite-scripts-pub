{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# @formatter:off\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.ensemble\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# @formatter:on\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# @formatter:off\n",
    "files = !ls data/*csv.jsonl\n",
    "# @formatter:on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "files = [f for f in files if 'sample' not in f and 'augmented' not in f and 'processed' not in f]\n",
    "assert len(files) == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "def load_json(d):\n",
    "    try:\n",
    "        return json.loads(d, strict=False)\n",
    "    except Exception as e:\n",
    "        print(\"FEK\")\n",
    "        print(d)\n",
    "        print(unidecode(d))\n",
    "        return json.loads(unidecode(d), strict=False)\n",
    "\n",
    "\n",
    "def load_jsonl(path: str):\n",
    "    print(\"loading\", path)\n",
    "    with open(path) as f:\n",
    "        return [load_json(d) for d in tqdm(f.readlines())]\n",
    "\n",
    "\n",
    "datasets = [load_jsonl(p) for p in files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def fix_bbc(d):\n",
    "    soup = BeautifulSoup(d['html_article'])\n",
    "    texts = [t for t in soup.find_all(attrs={'data-component': 'text-block'}) if\n",
    "             t.text.lower() not in (\"read more from reality check\", \"send us your questions\")]\n",
    "    article = '\\n'.join(d.text for d in texts)\n",
    "    links = [a['href'] for t in texts for a in t.find_all(name='a')]\n",
    "    d['text_article'] = article\n",
    "    d['links_article'] = links\n",
    "    return d\n",
    "\n",
    "\n",
    "datasets[0] = [fix_bbc(d) for d in datasets[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fix_factchck(d):\n",
    "    soup = BeautifulSoup(d['html_article'])\n",
    "    header = [t for t in soup.find_all(name='h3') if t.text]\n",
    "    texts = [t for t in soup.find_all(name='p') if t.text]\n",
    "    article = '\\n'.join(d.text for d in header + texts)\n",
    "    #links = [a['href'] for t in texts for a in t.find_all(name='a')]\n",
    "    d['text_article'] = article\n",
    "    #d['links_article'] = links\n",
    "    return d\n",
    "\n",
    "\n",
    "datasets[0] = [fix_factchck(d) for d in datasets[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def fix_snopes(d):\n",
    "    new_d = deepcopy(d)\n",
    "    soup = BeautifulSoup(d['html_article'])\n",
    "    soup.extract('script')\n",
    "    texts = []\n",
    "    for t in soup.find_all(name='p'):\n",
    "        texts.append(t)\n",
    "\n",
    "    article = '\\n'.join(t.text.strip() for t in texts)\n",
    "    #links = [a['href'] for t in texts for a in t.find_all(name='a')]\n",
    "    new_d['text_article'] = article\n",
    "    return new_d\n",
    "\n",
    "\n",
    "# print(datasets[6][1336]['text_article'])\n",
    "# print(20*'----')\n",
    "datasets[6] = [fix_snopes(d) for d in datasets[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fix_wp(d):\n",
    "    new_d = deepcopy(d)\n",
    "    soup = BeautifulSoup(d['html_article'])\n",
    "    texts = [t for t in soup.find_all(attrs={'data-qa': ['drop-cap-letter', 'article-header']}) if\n",
    "             t.text.lower() not in (\"read more from reality check\", \"send us your questions\")]\n",
    "    article = '\\n'.join(d.text for d in texts if not any(\n",
    "        d.text.lower().startswith(n) for n in (\"send us facts to check\", \"sign up for the fact checker\", \"the fact checker is\", \"(about our rating\")))\n",
    "    new_d['text_article'] = article\n",
    "    return new_d\n",
    "\n",
    "\n",
    "datasets[7] = [fix_wp(d) for d in datasets[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(datasets[7][1336]['html_article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "coarse_map = {\n",
    "    'TRUE': ['accurate', 'correct', 'true', 'legit'],\n",
    "    'FALSE': ['inaccurate', 'unsupported', 'flawed reasoning', 'incorrect', 'lacks context', 'false', 'wrong', 'scam', 'falso', 'fake',\n",
    "              'manipulated image', 'altered video', 'doctored image', 'hoax', 'faux', 'altered image', 'pants on fire', 'full flop',\n",
    "              'legend' 'four pinocchios'],\n",
    "    'MISLEADING': ['misleading', 'lacks context', 'missing context', 'misleading context', 'misattributed', 'out of context', 'exaggerated',\n",
    "                   'exaggeration', 'unsubstantiated', 'outdated'],\n",
    "    'ALMOST': ['imprecise', 'mostly correct', 'mostly accurate', 'correct butâ€¦', 'mostly true', 'lacks evidence', 'largely correct',\n",
    "               'largely accurate', 'close to accurate', 'one pinocchio'],\n",
    "    'HALF': ['mixture', 'mixed', 'half true', 'partly false', 'half-right, half-wrong', 'half flip', 'partially accurate', 'two pinocchios',\n",
    "             'half flop'],\n",
    "    'HARDLY': ['partly false', 'mostly false', 'three pinocchios'],\n",
    "    'SATIRE': ['satire', 'false satire', 'april fool', 'originated as satire', 'labelled satire']\n",
    "}\n",
    "\n",
    "def replace_label(label):\n",
    "    for k, v in coarse_map.items():\n",
    "        if label in v:\n",
    "            return k\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "\n",
    "def process_label(label):\n",
    "    label = str(label).lower().replace('_', ' ').replace('- ', '').replace('.', '').replace('this', '').replace('fasle', 'false')\n",
    "    label = label.replace('may be', '').replace('the', '').replace('claim', '').replace(' is ', ' ').replace('just', '').replace('flat', '').strip()\n",
    "    if len(label) < 40:\n",
    "        if any(label.startswith(n) for n in ['no', 'false']):\n",
    "            return 'false'\n",
    "        elif any(c in label for c in ['misleading', 'context']):\n",
    "            return 'misleading'\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def barh_from_counter(ctrs):\n",
    "    index = ['TRUE', 'ALMOST', 'HALF', 'HARDLY', 'FALSE', 'MISLEADING', 'SATIRE']\n",
    "    df = pd.DataFrame({k: [v.get(i, 0) for i in index] for k, v in ctrs.items()}, index)\n",
    "    df.plot.barh()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def label_distribution_ds(ds, name, cutoff=1):\n",
    "    labels = [replace_label(process_label(d['cR_textualRating'])) for d in ds]\n",
    "    full_ctr = Counter(labels)\n",
    "    ctr = {x: y for x, y in full_ctr.most_common() if y >= cutoff}\n",
    "    print('Dataset:', name)\n",
    "    print(len(ctr))\n",
    "    return ctr\n",
    "\n",
    "\n",
    "ctrs = dict()\n",
    "for ds, f in zip(datasets, files):\n",
    "\n",
    "    if all(n not in f for n in ['fullfact', 'bbc']):\n",
    "        ctrs[f] = label_distribution_ds(ds, f)\n",
    "\n",
    "barh_from_counter(ctrs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def augment_labels(ds):\n",
    "    for d in ds:\n",
    "        d['label'] = replace_label(process_label(d['cR_textualRating']))\n",
    "\n",
    "\n",
    "for ds, f in zip(datasets, files):\n",
    "    if all(n not in f for n in ['fullfact', 'bbc']):\n",
    "        augment_labels(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def does_output_make_sense(ds, name, k=5):\n",
    "    print('Dataset:', name)\n",
    "    for d in random.sample(ds, k):\n",
    "        print(' rating:', d['cR_textualRating'])\n",
    "        print(' title:', d['cR_title'])\n",
    "        print()\n",
    "    print('----' * 20)\n",
    "\n",
    "\n",
    "for ds, f in zip(datasets, files):\n",
    "    does_output_make_sense(ds, f, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def augment_explanation(ds, concat=False):\n",
    "    for d in ds:\n",
    "        if concat:\n",
    "            explanation = '. '.join((str(d['cR_textualRating']), d['cR_title']))\n",
    "        else:\n",
    "            explanation = str(d['cR_textualRating'])\n",
    "        d['explanation'] = explanation\n",
    "\n",
    "\n",
    "for ds, f in zip(datasets, files):\n",
    "    augment_explanation(ds, concat=any(n in f for n in ['climatefeedback', 'factcheck']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from handystuff.loaders import write_jsonl\n",
    "\n",
    "\n",
    "def write_out_ds(ds, f):\n",
    "    f = f.replace('data/', 'data/augmented-')\n",
    "    print(f)\n",
    "    write_jsonl(ds, f)\n",
    "\n",
    "\n",
    "for ds, f in zip(datasets, files):\n",
    "    write_out_ds(ds, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@formatter:off\n",
    "!cd data/ && for i in augmented-bbc.csv.jsonl augmented-climatefeedback.csv.jsonl augmented-factcheck.csv.jsonl augmented-fullfact.csv.jsonl augmented-nytimes.csv.jsonl augmented-politifact.csv.jsonl augmented-snopes.csv.jsonl augmented-washingtonpost.csv.jsonl; do cat $i | mlr --ijsonl --ojsonl cut -f text,text_article,label,explanation > processed-$i; done\n",
    "#@formatter:on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('smaite')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e59854198b62e77d92da15d0c036c042ba0ed0a9b0aaffcd75c388e7e8858f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
